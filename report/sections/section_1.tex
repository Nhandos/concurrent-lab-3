%----------------------------------------------------------------------------------------
%	Monte Carlo method for estimating Pi using OpenMP
%----------------------------------------------------------------------------------------

\section{Monte Carlo method for approximating the value of $\pi$ in OpenMP}

\subsection{Compile and run the template code with a single thread, and thus verify that the serial
version is correct.}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{graphics/P1_a_terminal_output.png}
    \caption{Terminal output of execution of template code with a single thread}
    \label{fig:P1_a_term_out}
\end{figure}

\subsection{Parallelize the code for multi-threaded execution by ONLY adding OpenMP directives}

\vspace{0.5cm}
\lstinputlisting[
	style=CStyle,
	firstline=34, % First line of code
	lastline=48, % Lastl ine of code
	caption=Parallizing the code for multi-threaded execution (line 34-48 in lab3part1.c), % Caption above the listing
	label=lst:lab3part1a, % Label for referencing this listing
	frame=single, % Frame around the code listing
	showstringspaces=false, % Don't put marks in string spaces
	numbers=left, % Line numbers on left
	numberstyle=\normalsize % Line numbers styling
    ]{../code/lab3part1.c}
    
\subsubsection{At what point in the code new threads are forked and joined?}
The code is serial up until \emph{pragma}. At pragma, multiple threads are forked,
line 1 in \cref{lst:lab3part1a}. After the structured block (line 2 - 15), the slaves are joined with 
the master thread and execution resumes on the master thread.

\subsubsection{Which part of the code is executed by which threads?}
The total number of threads are comprised of one main thread, and the remainder are slave threads.
The main thread will run all code outside of the parallel enivornment. Master and slaves thread will 
run the section of code assigned by the OpenMP directives - line 2 - 15.

\subsubsection{Where barrires if any in terms of thread synchronization would be encountered,
and hence how the code achives parallelization}

Thread synchronization is needed to so reduction can be apply to the variable
\emph{number\_in\_circle} to avoid race condition. OpenMP's reduction basically
behaves similarly to a critical section in terms of synchronisation.


\subsection{Tabulate the speedup and efficiency for 1,2,4,8 threads.}

\begin{center}
\begin{tabular}{|| c | c | c | c ||}
	\hline
	Threads & Time (s) & Speedup & Efficiency (\%) \\ [0.5ex]
	\hline 
	1 & 20.356 & N/A & 100 \\
	2 & 10.376 & 1.962 & 98.10 \\
	4 & 5.908 & 3.476 & 86.90 \\
	8 & 3.498 & 5.819 & 72.74 \\
	\hline
\end{tabular}
\end{center}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{graphics/P1_c_terminal_output.PNG}
	\caption{Terminal output for execution with 1, 2, 4, 8 threads respectively for
	estimating $pi$ using Monte Carlo's method with $10^9$ samples}
	\label{fig:lab3part1c}
\end{figure}

\subsubsection{Do the result match your expectation? Why?}

The result matches our expectation. The speedup will increase as the number
of threads increase but will saturate at somepoint due to Amdahl's Law which
states that the parallel program is bounded by the section of code that 
can only be executed in serial. Hence, even though we are allocating more
threads, the efficiency decreases and we get a diminishing return due to this
fact.